{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering process for policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code permits the culstering of policies.\n",
    "\n",
    "It uses Sentence BERT as an embedder and HDBSCAN as a clustering algorithm. \n",
    "- Sentence BERT is a Python module for accessing, using, and training state-of-the-art text and image embedding models. It can be used to compute embeddings using Sentence Transformer models (quickstart) or to calculate similarity scores using Cross-Encoder models.\n",
    "- HDBSCAN is a clustering algorithm extending DBSCAN by converting it into a hierarchical clustering algorithm. DBSCAN is a density-based clustering method that finds core samples and expands clusters from them. \n",
    "\n",
    "In this Jupyter Notebook we will: \n",
    "1. Import the data retrieved from the policy extraction process ; \n",
    "2. Import the relevant packages ;\n",
    "3. Prepare data for clustering ;\n",
    "4. Cluster with HDBSCAN ; \n",
    "5. Re-process the clusters ; \n",
    "    1. Export for manual check ;\n",
    "    2. Reclustering with HDBSCAN ;\n",
    "    3. Clean and name the final clusters ;\n",
    "6. Export the policy clustering data. \n",
    "\n",
    "To complete those tasks you will need:\n",
    "- The dataset of papers with the policy extraction of 1_policy_extraction code. \n",
    "\n",
    "At the end of this script you will extract: \n",
    "- The named_cluster_df dataset of policy clusters and the sentences extracted during the 1_policy_extraction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the data retrieved from the screening process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the input and output access paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output dataset of the 1_policy_extraction\n",
    "input_path = \"\"\n",
    "\n",
    "# 3 outputs are needed\n",
    "## First to export the first HDBSCAN results to check by hand\n",
    "output_path_first_manual_check = \"\"\n",
    "## Second checking after reprocessing\n",
    "output_path_second_manual_check = \"\"\n",
    "## Final extraction\n",
    "export_path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\easycash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\easycash\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Packages for HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare data for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "df = pd.read_csv(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and parse JSON\n",
    "def clean_and_parse_json(json_string):\n",
    "    try:\n",
    "        # Skip invalid strings\n",
    "        if not json_string.strip().startswith(\"{\"):\n",
    "            return None\n",
    "        # Remove trailing commas\n",
    "        cleaned_string = re.sub(r\",\\s*}\", \"}\", json_string)\n",
    "        cleaned_string = re.sub(r\",\\s*]\", \"]\", cleaned_string)\n",
    "        # Parse the cleaned JSON string\n",
    "        return json.loads(cleaned_string)\n",
    "    except json.JSONDecodeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the JSON is meaningful\n",
    "def is_meaningful_json(parsed_data):\n",
    "    if not isinstance(parsed_data, dict):\n",
    "        return False\n",
    "    # Check if the JSON only contains \"None\" values\n",
    "    for key, value in parsed_data.items():\n",
    "        if key != \"None\" or (isinstance(value, dict) and any(k != \"None\" for k in value.keys())):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_items(dataframe):\n",
    "    new_rows = []\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        extracted_data = row['extracted_features_and_correlations']\n",
    "        \n",
    "        # Skip rows with \"No abstract\" or invalid data\n",
    "        if extracted_data == \"No abstract\" or not isinstance(extracted_data, str):\n",
    "            continue\n",
    "        \n",
    "        # Clean and parse JSON content\n",
    "        parsed_data = clean_and_parse_json(extracted_data)\n",
    "        if parsed_data is None or not is_meaningful_json(parsed_data):\n",
    "            print(f\"Skipping non-meaningful JSON for index {idx}\")\n",
    "            continue\n",
    "        \n",
    "        geographic = parsed_data.get(\"GEOGRAPHIC\", \"None\")\n",
    "        \n",
    "        # Iterate through the items\n",
    "        for item, details in parsed_data.items():\n",
    "            if item == \"GEOGRAPHIC\":  # Skip the geographic key\n",
    "                continue\n",
    "            \n",
    "            # Ensure details is a dictionary before accessing keys\n",
    "            if not isinstance(details, dict):\n",
    "                print(f\"Skipping invalid details for item {item} at index {idx}: {details}\")\n",
    "                continue\n",
    "\n",
    "            actor = details.get(\"ACTOR\", \"None\")\n",
    "            mode = details.get(\"MODE\", \"None\")\n",
    "            population = details.get(\"POPULATION\", \"None\")\n",
    "            \n",
    "            # Append new row for each ITEM\n",
    "            new_rows.append({\n",
    "                'index': idx,  # Use original index as reference\n",
    "                'GEOGRAPHIC': geographic,\n",
    "                'ITEM': item,\n",
    "                'ACTOR': actor,\n",
    "                'MODE': mode,\n",
    "                'POPULATION': population\n",
    "            })\n",
    "    \n",
    "    # Create new DataFrame\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    # Create a new index with suffix for duplicates\n",
    "    new_df['new_index'] = new_df.groupby('index').cumcount().add(1).astype(str)\n",
    "    new_df['new_index'] = new_df['index'].astype(str) + \"_\" + new_df['new_index']\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the extraction\n",
    "extracted_items_df = extract_items(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_columns(df):\n",
    "    # Define a function to concatenate values if they are not \"None\"\n",
    "    def concatenate(row):\n",
    "        values = [row['ITEM'], row['MODE']]\n",
    "        # Filter out \"None\" values and join with a space\n",
    "        return \" \".join(str(value) for value in values if value != \"None\")\n",
    "    \n",
    "    # Apply the function to each row and create a new column\n",
    "    df['concatenated_column'] = df.apply(concatenate, axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply the function to the extracted_items_df\n",
    "updated_df = concatenate_columns(extracted_items_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cluster with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_corpus = updated_df['concatenated_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Smaller Model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2: Ensure Preprocessed Corpus has a Continuous Index\n",
    "preprocessed_corpus = preprocessed_corpus.reset_index(drop=True)\n",
    "\n",
    "# Convert to a list for parallel processing\n",
    "corpus_list = preprocessed_corpus.tolist()\n",
    "\n",
    "# Step 3: Batch Embedding Function\n",
    "def embed_batch(batch):\n",
    "    return embedder.encode(batch, show_progress_bar=False)\n",
    "\n",
    "# Step 4: Generate Embeddings in Batches (Parallelized)\n",
    "def parallel_embedding(corpus, batch_size=512):\n",
    "    embeddings = Parallel(n_jobs=-1)(\n",
    "        delayed(embed_batch)(corpus[i:i + batch_size])\n",
    "        for i in range(0, len(corpus), batch_size)\n",
    "    )\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Encode the corpus in parallel\n",
    "batch_size = 512\n",
    "corpus_embeddings = parallel_embedding(corpus_list, batch_size=batch_size)\n",
    "\n",
    "# Step 5: Apply Dimensionality Reduction (PCA) Before Normalization\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "reduced_embeddings = pca.fit_transform(corpus_embeddings)\n",
    "reduced_embeddings = normalize(reduced_embeddings)\n",
    "\n",
    "# Step 6: Apply HDBSCAN Clustering\n",
    "# HDBSCAN automatically determines the number of clusters\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50,  # Minimum cluster size\n",
    "    min_samples=10,        # Minimum samples in a neighborhood for a core point\n",
    "    metric='euclidean',   # Distance metric\n",
    "    cluster_selection_epsilon=0.5  # Adjust for fine-grained clustering\n",
    ")\n",
    "cluster_assignment = hdbscan_model.fit_predict(reduced_embeddings)\n",
    "\n",
    "# Step 7: Analyze and Visualize Clusters\n",
    "# HDBSCAN assigns -1 to noise points\n",
    "num_clusters_found = len(set(cluster_assignment)) - (1 if -1 in cluster_assignment else 0)\n",
    "print(f\"Number of clusters found: {num_clusters_found}\")\n",
    "\n",
    "# Group sentences by cluster\n",
    "clustered_sentences = [[] for _ in range(num_clusters_found)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    if cluster_id != -1:  # Exclude noise points\n",
    "        clustered_sentences[cluster_id].append(corpus_list[sentence_id])\n",
    "\n",
    "# Print clusters\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(f\"Cluster {i + 1}:\")\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Re-process the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Export for manual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extraction of a random sample of sentences to validation\n",
    "\n",
    "# Suppress duplicate sentences\n",
    "unique_clustered_sentences = [\n",
    "    list(set(cluster)) for cluster in clustered_sentences\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "data = []\n",
    "for cluster_num, sentences in enumerate(unique_clustered_sentences):\n",
    "    # Get the number of sentences in the cluster\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    # Randomly sample 15 sentences (or fewer if the cluster has less than 10 sentences)\n",
    "    sample_sentences = random.sample(sentences, min(15, num_sentences))\n",
    "    \n",
    "    # Append the cluster info to the data list\n",
    "    data.append({\n",
    "        \"Cluster Number\": cluster_num + 1,\n",
    "        \"Number of Sentences\": num_sentences,\n",
    "        \"Sample Sentences\": \"; \".join(sample_sentences)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "cluster_summary_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the clusters for manual check\n",
    "cluster_summary_df.to_csv(output_path_first_manual_check, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reclustering with HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing the clusters to subdivide (here clusters 6 and 11), use HDBSCAN to recluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-clsutering of subdivision of clusters\n",
    "### List of clusters to subdivide\n",
    "clusters_to_subdivide = [6, 11]\n",
    "\n",
    "# Function to subdivide clusters and create a new dataframe\n",
    "def subdivide_clusters_to_new_dataframe(clustered_sentences, cluster_assignment, reduced_embeddings, clusters_to_subdivide):\n",
    "    # Create a list for the final combined clusters\n",
    "    combined_clusters = []\n",
    "    new_subclusters = []  # To hold subdivided clusters\n",
    "\n",
    "    # Add clusters that are not being subdivided to the final list\n",
    "    for cluster_id, sentences in enumerate(clustered_sentences):\n",
    "        if (cluster_id + 1) not in clusters_to_subdivide:  # Adjust for 1-based indexing in `clusters_to_subdivide`\n",
    "            combined_clusters.append({\"cluster_id\": cluster_id + 1, \"sentences\": sentences})\n",
    "\n",
    "    # Subdivide the specified clusters\n",
    "    for cluster_index in clusters_to_subdivide:\n",
    "        # Adjust index for 0-based indexing (Python lists)\n",
    "        cluster_id = cluster_index - 1\n",
    "\n",
    "        # Extract the embeddings and sentences for the current cluster\n",
    "        indices = [i for i, cid in enumerate(cluster_assignment) if cid == cluster_id]\n",
    "        if len(indices) < 5:  # HDBSCAN needs at least a few points\n",
    "            continue\n",
    "\n",
    "        cluster_embeddings = reduced_embeddings[indices]\n",
    "        cluster_sentences = [preprocessed_corpus[i] for i in indices]\n",
    "\n",
    "        # Apply HDBSCAN to subdivide the cluster\n",
    "        # Apply HDBSCAN to subdivide the cluster\n",
    "        hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=5,  # Minimum cluster size\n",
    "            min_samples=5,        # Minimum samples in a neighborhood for a core point\n",
    "            metric='euclidean',   # Distance metric\n",
    "            cluster_selection_epsilon=0.45  # Adjust for fine-grained clustering\n",
    "            )\n",
    "        hdbscan_labels = hdbscan_model.fit_predict(cluster_embeddings)\n",
    "\n",
    "        # Map each HDBSCAN cluster to the combined list\n",
    "        for hdbscan_cluster_id in set(hdbscan_labels):\n",
    "            if hdbscan_cluster_id == -1:  # Skip noise\n",
    "                continue\n",
    "            new_subclusters.append(\n",
    "                {\n",
    "                    \"cluster_id\": f\"{cluster_index}-{hdbscan_cluster_id}\",\n",
    "                    \"sentences\": [cluster_sentences[i] for i, label in enumerate(hdbscan_labels) if label == hdbscan_cluster_id],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Append subdivided clusters to the remaining clusters\n",
    "    combined_clusters.extend(new_subclusters)\n",
    "\n",
    "    # Convert the combined clusters into a dataframe\n",
    "    new_cluster_df = pd.DataFrame(combined_clusters)\n",
    "    return new_cluster_df\n",
    "\n",
    "# Subdivide selected clusters and create a new dataframe\n",
    "new_cluster_df = subdivide_clusters_to_new_dataframe(\n",
    "    clustered_sentences, cluster_assignment, reduced_embeddings, clusters_to_subdivide\n",
    ")\n",
    "\n",
    "# Display the new dataframe\n",
    "print(new_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your desired output path\n",
    "new_cluster_df.to_csv(output_path_second_manual_check, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clean and name the final clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean by suppressing the clusters off topic, merge and finally name the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppress_and_merge_clusters(cluster_df, clusters_to_suppress, clusters_to_merge):\n",
    "    \"\"\"\n",
    "    Suppress and merge clusters in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - cluster_df: DataFrame containing cluster information.\n",
    "    - clusters_to_suppress: List of cluster IDs to suppress.\n",
    "    - clusters_to_merge: Dictionary where keys are clusters to keep, and values are lists of clusters to merge into them.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with suppressed and merged clusters.\n",
    "    \"\"\"\n",
    "    # Suppress clusters\n",
    "    suppressed_df = cluster_df[~cluster_df[\"cluster_id\"].isin(clusters_to_suppress)]\n",
    "\n",
    "    # Merge clusters\n",
    "    for target_cluster, clusters_to_merge_into in clusters_to_merge.items():\n",
    "        # Find the sentences for all clusters to merge\n",
    "        sentences_to_merge = []\n",
    "        for merge_cluster in clusters_to_merge_into:\n",
    "            merge_rows = suppressed_df[suppressed_df[\"cluster_id\"] == merge_cluster]\n",
    "            if not merge_rows.empty:\n",
    "                sentences_to_merge.extend(merge_rows.iloc[0][\"sentences\"])\n",
    "        \n",
    "        # Append sentences to the target cluster\n",
    "        target_row = suppressed_df[suppressed_df[\"cluster_id\"] == target_cluster]\n",
    "        if not target_row.empty:\n",
    "            target_row_index = target_row.index[0]\n",
    "            suppressed_df.at[target_row_index, \"sentences\"] = (\n",
    "                suppressed_df.at[target_row_index, \"sentences\"] + sentences_to_merge\n",
    "            )\n",
    "        \n",
    "        # Remove merged clusters\n",
    "        suppressed_df = suppressed_df[~suppressed_df[\"cluster_id\"].isin(clusters_to_merge_into)]\n",
    "    \n",
    "    # Reset the index for a clean DataFrame\n",
    "    suppressed_df.reset_index(drop=True, inplace=True)\n",
    "    return suppressed_df\n",
    "\n",
    "\n",
    "# Define clusters to suppress\n",
    "clusters_to_suppress = [\n",
    "    1, \"11-0\"\n",
    "]\n",
    "\n",
    "# Define clusters to merge\n",
    "clusters_to_merge = {\n",
    "    17: [18],\n",
    "    38: [39]\n",
    "}\n",
    "\n",
    "# Apply suppression and merging\n",
    "cleaned_cluster_df = suppress_and_merge_clusters(new_cluster_df, clusters_to_suppress, clusters_to_merge)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(cleaned_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_sentences(cluster_df):\n",
    "    \"\"\"\n",
    "    Removes duplicate sentences within each row of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - cluster_df: DataFrame containing cluster information with a 'sentences' column.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with unique sentences in each cluster.\n",
    "    \"\"\"\n",
    "    # Ensure each row's \"sentences\" list contains only unique values\n",
    "    cluster_df[\"sentences\"] = cluster_df[\"sentences\"].apply(lambda x: list(set(x)))\n",
    "    return cluster_df\n",
    "\n",
    "\n",
    "# Apply the function to remove duplicates\n",
    "cleaned_cluster_df = remove_duplicate_sentences(cleaned_cluster_df)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(cleaned_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_cluster_names(cluster_df, cluster_name_mapping):\n",
    "    \"\"\"\n",
    "    Assign cluster names based on the provided mapping.\n",
    "\n",
    "    Parameters:\n",
    "    - cluster_df: DataFrame containing clusters.\n",
    "    - cluster_name_mapping: Dictionary mapping cluster_id to cluster names.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with a new column for cluster names.\n",
    "    \"\"\"\n",
    "    cluster_df = cluster_df.copy()  # Avoid modifying the original DataFrame\n",
    "\n",
    "    # Assign names using the mapping\n",
    "    cluster_df[\"Cluster Name\"] = cluster_df[\"cluster_id\"].map(cluster_name_mapping)\n",
    "\n",
    "    # Fill missing names with \"Unnamed Cluster\"\n",
    "    cluster_df[\"Cluster Name\"].fillna(\"Unnamed Cluster\", inplace=True)\n",
    "\n",
    "    return cluster_df\n",
    "\n",
    "\n",
    "# Define the cluster name mapping (shortened for brevity; use the full mapping provided)\n",
    "cluster_name_mapping = {\n",
    "    10: \"Public or Private Investments\",  \n",
    "    14: \"Carpooling\"\n",
    "}\n",
    "\n",
    "# Apply the function to assign names\n",
    "named_cluster_df = assign_cluster_names(cleaned_cluster_df, cluster_name_mapping)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(named_cluster_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export the policy clustering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your desired output path\n",
    "named_cluster_df.to_csv(export_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
