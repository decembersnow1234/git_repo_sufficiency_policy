{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering process for outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code allows the culstering of outcomes and the cleaning of correlations.\n",
    "\n",
    "It uses Sentence BERT as an embedder and HDBSCAN as a clustering algorithm. \n",
    "- Sentence BERT is a Python module for accessing, using, and training state-of-the-art text and image embedding models. It can be used to compute embeddings using Sentence Transformer models (quickstart) or to calculate similarity scores using Cross-Encoder models.\n",
    "- HDBSCAN is a clustering algorithm extending DBSCAN by converting it into a hierarchical clustering algorithm. DBSCAN is a density-based clustering method that finds core samples and expands clusters from them. \n",
    "\n",
    "In this Jupyter Notebook we will: \n",
    "1. Import the data retrieved from the policy extraction process ; \n",
    "2. Import the relevant packages ;\n",
    "3. Prepare data for clustering ;\n",
    "    1. Filtering ;\n",
    "4. Cluster with HDBSCAN ; \n",
    "5. Re-process the clusters ; \n",
    "    1. Export for manual check ;\n",
    "    2. Reclustering with HDBSCAN ;\n",
    "    3. Clean and name the final clusters ;\n",
    "6. Meta clustering. \n",
    "\n",
    "To complete those tasks you will need:\n",
    "- The dataset of papers with the policy extraction of the 1_policy_extraction code. \n",
    "- The dataset of papers with the clustered policy of the 2_policy_clustering code. \n",
    "\n",
    "At the end of this script you will extract: \n",
    "- The named_cluster_df dataset of outcome clusters and the sentences extracted during the 1_policy_extraction. \n",
    "- The prepared dataset for the data analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the data retrieved from the screening process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the input and output access paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 inputs\n",
    "## Output dataset of the 1_policy_extraction\n",
    "input_path = \"\"\n",
    "## Output dataset of the 2_policy_clustering\n",
    "input_path_cluster_policy = \"\"\n",
    "\n",
    "# 6 outputs are needed\n",
    "## The extracted data from the JSON format\n",
    "policy_cluster_outcomes_raw = \"\"\n",
    "## The updated dataset ready for clustering\n",
    "prepared_for_clustering  = \"\" \n",
    "## First to export the first HDBSCAN results to check by hand\n",
    "output_path_first_manual_check = \"\"\n",
    "## Second checking after reprocessing\n",
    "output_path_second_manual_check = \"\"\n",
    "## Final dataset with clusters\n",
    "export_final_dataset_cluster = \"\"\n",
    "## Aggregated data on clusters\n",
    "export_agg_cluster = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # Import the ast module to parse string representations of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare data for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and parse JSON\n",
    "def clean_and_parse_json(json_string):\n",
    "    try:\n",
    "        # Skip invalid strings\n",
    "        if not json_string.strip().startswith(\"{\"):\n",
    "            return None\n",
    "        # Remove trailing commas\n",
    "        cleaned_string = re.sub(r\",\\s*}\", \"}\", json_string)\n",
    "        cleaned_string = re.sub(r\",\\s*]\", \"]\", cleaned_string)\n",
    "        # Parse the cleaned JSON string\n",
    "        return json.loads(cleaned_string)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "# Function to check if the JSON is meaningful\n",
    "def is_meaningful_json(parsed_data):\n",
    "    if not isinstance(parsed_data, dict):\n",
    "        return False\n",
    "    # Check if the JSON only contains \"None\" values\n",
    "    for key, value in parsed_data.items():\n",
    "        if key != \"None\" or (isinstance(value, dict) and any(k != \"None\" for k in value.keys())):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_items(dataframe):\n",
    "    new_rows = []\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        extracted_data = row['extracted_features_and_correlations']\n",
    "        \n",
    "        # Skip rows with \"No abstract\" or invalid data\n",
    "        if extracted_data == \"No abstract\" or not isinstance(extracted_data, str):\n",
    "            continue\n",
    "        \n",
    "        # Clean and parse JSON content\n",
    "        parsed_data = clean_and_parse_json(extracted_data)\n",
    "        if parsed_data is None or not is_meaningful_json(parsed_data):\n",
    "            print(f\"Skipping non-meaningful JSON for index {idx}\")\n",
    "            continue\n",
    "        \n",
    "        geographic = parsed_data.get(\"GEOGRAPHIC\", \"None\")\n",
    "        policy_index = 0  # Initialize policy index for each row\n",
    "        \n",
    "        # Iterate through the items\n",
    "        for policy, details in parsed_data.items():\n",
    "            if policy == \"GEOGRAPHIC\":  # Skip the geographic key\n",
    "                continue\n",
    "            \n",
    "            # Ensure details is a dictionary before accessing keys\n",
    "            if not isinstance(details, dict):\n",
    "                print(f\"Skipping invalid details for policy {policy} at index {idx}: {details}\")\n",
    "                continue\n",
    "\n",
    "            actor = details.get(\"ACTOR\", \"None\")\n",
    "            mode = details.get(\"MODE\", \"None\")\n",
    "            population = details.get(\"POPULATION\", \"None\")\n",
    "            factors = details.get(\"FACTOR\", {})\n",
    "\n",
    "            policy_index += 1  # Increment policy index\n",
    "            factor_index = 0  # Initialize factor index for each policy\n",
    "\n",
    "            # Iterate through the factors\n",
    "            if isinstance(factors, dict):\n",
    "                for factor, corr_details in factors.items():\n",
    "                    correlation = corr_details.get(\"CORRELATION\", \"None\")\n",
    "                    factor_index += 1  # Increment factor index\n",
    "                    new_rows.append({\n",
    "                        'row_index': idx,  # Use original index as reference\n",
    "                        'policy_index': policy_index,\n",
    "                        'factor_index': factor_index,\n",
    "                        'GEOGRAPHIC': geographic,\n",
    "                        'POLICY': policy,\n",
    "                        'ACTOR': actor,\n",
    "                        'MODE': mode,\n",
    "                        'POPULATION': population,\n",
    "                        'FACTOR': factor,\n",
    "                        'CORRELATION': correlation\n",
    "                    })\n",
    "            else:\n",
    "                # If no factors, add a row for the policy only\n",
    "                factor_index += 1  # Ensure factor index is still incremented\n",
    "                new_rows.append({\n",
    "                    'row_index': idx,\n",
    "                    'policy_index': policy_index,\n",
    "                    'factor_index': factor_index,\n",
    "                    'GEOGRAPHIC': geographic,\n",
    "                    'POLICY': policy,\n",
    "                    'ACTOR': actor,\n",
    "                    'MODE': mode,\n",
    "                    'POPULATION': population,\n",
    "                    'FACTOR': \"None\",\n",
    "                    'CORRELATION': \"None\"\n",
    "                })\n",
    "    \n",
    "    # Create new DataFrame\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "    # Set multi-level index\n",
    "    new_df.set_index(['row_index', 'policy_index', 'factor_index'], inplace=True)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the extraction\n",
    "extracted_items_df = extract_items(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_columns(df):\n",
    "    # Define a function to concatenate values if they are not \"None\"\n",
    "    def concatenate(row):\n",
    "        values = [row['POLICY'], row['MODE']]\n",
    "        # Filter out \"None\" values and join with a space\n",
    "        return \" \".join(str(value) for value in values if value != \"None\")\n",
    "    \n",
    "    # Apply the function to each row and create a new column\n",
    "    df['concatenated_policy'] = df.apply(concatenate, axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply the function to the extracted_items_df\n",
    "updated_df = concatenate_columns(extracted_items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.read_csv(input_path_cluster_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'sentences' column in cluster_df is treated as a list\n",
    "cluster_df['sentences'] = cluster_df['sentences'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each row in updated_df\n",
    "for index, row in updated_df.iterrows():\n",
    "    policy = row['concatenated_policy']\n",
    "    matched_clusters = []  # Collect all matched clusters for the policy\n",
    "\n",
    "    # Check each cluster in cluster_df\n",
    "    for _, cluster_row in cluster_df.iterrows():\n",
    "        # Ensure exact match of the policy in the cluster's sentences list\n",
    "        if policy in cluster_row['sentences']:\n",
    "            matched_clusters.append(cluster_row['Cluster Name'])\n",
    "    \n",
    "    # If matches are found, create duplicate rows\n",
    "    if matched_clusters:\n",
    "        for cluster_index, cluster in enumerate(matched_clusters, start=1):\n",
    "            new_row = row.copy()  # Copy the original row\n",
    "            new_row['matched_cluster'] = cluster\n",
    "            new_row['cluster_index'] = cluster_index  # Assign integer value\n",
    "            results.append(new_row)  # Add the new row to the results\n",
    "    else:\n",
    "        # If no match is found, add the original row with default cluster_index as 1\n",
    "        row['matched_cluster'] = None\n",
    "        row['cluster_index'] = 1  # Default index\n",
    "        results.append(row)\n",
    "\n",
    "# Create a new DataFrame from the results\n",
    "expanded_df = pd.DataFrame(results)\n",
    "\n",
    "# Ensure the cluster_index column is an integer type\n",
    "expanded_df['cluster_index'] = expanded_df['cluster_index'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = expanded_df.reset_index().rename(columns={\"level_0\":\"row_index\",\"level_1\":\"policy_index\",\"level_2\":\"factor_index\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your desired output path\n",
    "expanded_df.to_csv(policy_cluster_outcomes_raw, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter None values and homoogeneity of CORRELATION values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df = pd.read_csv(policy_cluster_outcomes_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows using drop()\n",
    "expanded_df.drop(expanded_df[expanded_df['CORRELATION'] == 'None'].index, inplace = True)\n",
    "expanded_df.drop(expanded_df[expanded_df['CORRELATION'] == 'none'].index, inplace = True)\n",
    "\n",
    "expanded_df.dropna(subset=['matched_cluster'] , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notes Louis:\n",
    "- are these exclusive lists of keywords? Meaning if it doesn't find those exact words it won't consider that there is an impact mentioned?\n",
    "- How do we differentiate between items where decreasing is a good or a bad thing? ex: decreasing car use vs bike use\n",
    "\"\"\"\n",
    "\n",
    "list_decrease = [ 'competing', 'regressive', 'worsening']\n",
    "list_increase = ['improving', 'enhancing', 'improving', 'increasing in Germany','increasing in Germany, decreasing in California', 'reproduced or disrupted', 'U-shaped']\n",
    "list_neutral = ['changing', 'competing', 'impact', 'impacted', 'influenced', 'influencing', 'may increase or decrease', 'mixed', 'stable', 'sustaining', 'transformations', 'transforming', 'variability', 'varying']\n",
    "\n",
    "expanded_df.loc[expanded_df['CORRELATION'].isin(list_decrease),'CORRELATION'] = 'decreasing'\n",
    "expanded_df.loc[expanded_df['CORRELATION'].isin(list_increase),'CORRELATION'] = 'increasing'\n",
    "expanded_df.loc[expanded_df['CORRELATION'].isin(list_neutral),'CORRELATION'] = 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your desired output path\n",
    "expanded_df.to_csv(prepared_for_clustering, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cluster with HDBSCAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_cluster_factor_filtered = pd.read_csv(prepared_for_clustering)\n",
    "preprocessed_corpus = policy_cluster_factor_filtered['FACTOR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Smaller Model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2: Ensure Preprocessed Corpus has a Continuous Index\n",
    "preprocessed_corpus = preprocessed_corpus.reset_index(drop=True)\n",
    "\n",
    "# Convert to a list for parallel processing\n",
    "corpus_list = preprocessed_corpus.tolist()\n",
    "\n",
    "# Step 3: Batch Embedding Function\n",
    "def embed_batch(batch):\n",
    "    return embedder.encode(batch, show_progress_bar=False)\n",
    "\n",
    "# Step 4: Generate Embeddings in Batches (Parallelized)\n",
    "def parallel_embedding(corpus, batch_size=512):\n",
    "    embeddings = Parallel(n_jobs=-1)(\n",
    "        delayed(embed_batch)(corpus[i:i + batch_size])\n",
    "        for i in range(0, len(corpus), batch_size)\n",
    "    )\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Encode the corpus in parallel\n",
    "batch_size = 512\n",
    "corpus_embeddings = parallel_embedding(corpus_list, batch_size=batch_size)\n",
    "\n",
    "# Step 5: Apply Dimensionality Reduction (PCA) Before Normalization\n",
    "pca = PCA(n_components=50, random_state=42)\n",
    "reduced_embeddings = pca.fit_transform(corpus_embeddings)\n",
    "reduced_embeddings = normalize(reduced_embeddings)\n",
    "\n",
    "# Step 6: Apply HDBSCAN Clustering\n",
    "# HDBSCAN automatically determines the number of clusters\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=50,  # Minimum cluster size\n",
    "    min_samples=10,        # Minimum samples in a neighborhood for a core point\n",
    "    metric='euclidean',   # Distance metric\n",
    "    cluster_selection_epsilon=0.5  # Adjust for fine-grained clustering\n",
    ")\n",
    "cluster_assignment = hdbscan_model.fit_predict(reduced_embeddings)\n",
    "\n",
    "# Step 7: Analyze and Visualize Clusters\n",
    "# HDBSCAN assigns -1 to noise points\n",
    "num_clusters_found = len(set(cluster_assignment)) - (1 if -1 in cluster_assignment else 0)\n",
    "print(f\"Number of clusters found: {num_clusters_found}\")\n",
    "\n",
    "# Group sentences by cluster\n",
    "clustered_sentences = [[] for _ in range(num_clusters_found)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    if cluster_id != -1:  # Exclude noise points\n",
    "        clustered_sentences[cluster_id].append(corpus_list[sentence_id])\n",
    "\n",
    "# Print clusters\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(f\"Cluster {i + 1}:\")\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Re-process the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Export for manual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Suppress duplicate sentences\n",
    "unique_clustered_sentences = [\n",
    "    list(set(cluster)) for cluster in clustered_sentences\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "data = []\n",
    "for cluster_num, sentences in enumerate(unique_clustered_sentences):\n",
    "    # Convert all sentences to strings\n",
    "    sentences = [str(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Get the number of sentences in the cluster\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    # Randomly sample 10 sentences (or fewer if the cluster has less than 10 sentences)\n",
    "    sample_sentences = random.sample(sentences, min(15, num_sentences))\n",
    "    \n",
    "    # Append the cluster info to the data list\n",
    "    data.append({\n",
    "        \"Cluster Number\": cluster_num + 1,\n",
    "        \"Number of Sentences\": num_sentences,\n",
    "        \"Sample Sentences\": \"; \".join(sample_sentences)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "cluster_summary_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your desired output path\n",
    "cluster_summary_df.to_csv(output_path_first_manual_check, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reclustering with HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing the clusters to subdivide (here clusters 4 and 17), use HDBSCAN to recluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of clusters to subdivide\n",
    "clusters_to_subdivide = [4, 17]\n",
    "\n",
    "\n",
    "# Function to subdivide clusters and create a new dataframe\n",
    "def subdivide_clusters_to_new_dataframe(clustered_sentences, cluster_assignment, reduced_embeddings, clusters_to_subdivide):\n",
    "    # Create a list for the final combined clusters\n",
    "    combined_clusters = []\n",
    "    new_subclusters = []  # To hold subdivided clusters\n",
    "\n",
    "    # Add clusters that are not being subdivided to the final list\n",
    "    for cluster_id, sentences in enumerate(clustered_sentences):\n",
    "        if (cluster_id + 1) not in clusters_to_subdivide:  # Adjust for 1-based indexing in `clusters_to_subdivide`\n",
    "            combined_clusters.append({\"cluster_id\": cluster_id + 1, \"sentences\": sentences})\n",
    "\n",
    "    # Subdivide the specified clusters\n",
    "    for cluster_index in clusters_to_subdivide:\n",
    "        # Adjust index for 0-based indexing (Python lists)\n",
    "        cluster_id = cluster_index - 1\n",
    "\n",
    "        # Extract the embeddings and sentences for the current cluster\n",
    "        indices = [i for i, cid in enumerate(cluster_assignment) if cid == cluster_id]\n",
    "        if len(indices) < 5:  # HDBSCAN needs at least a few points\n",
    "            continue\n",
    "\n",
    "        cluster_embeddings = reduced_embeddings[indices]\n",
    "        cluster_sentences = [preprocessed_corpus[i] for i in indices]\n",
    "\n",
    "        # Apply HDBSCAN to subdivide the cluster\n",
    "        # Apply HDBSCAN to subdivide the cluster\n",
    "        hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=5,  # Minimum cluster size\n",
    "            min_samples=5,        # Minimum samples in a neighborhood for a core point\n",
    "            metric='euclidean',   # Distance metric\n",
    "            cluster_selection_epsilon=0.45  # Adjust for fine-grained clustering\n",
    "            )\n",
    "        hdbscan_labels = hdbscan_model.fit_predict(cluster_embeddings)\n",
    "\n",
    "        # Map each HDBSCAN cluster to the combined list\n",
    "        for hdbscan_cluster_id in set(hdbscan_labels):\n",
    "            if hdbscan_cluster_id == -1:  # Skip noise\n",
    "                continue\n",
    "            new_subclusters.append(\n",
    "                {\n",
    "                    \"cluster_id\": f\"{cluster_index}-{hdbscan_cluster_id}\",\n",
    "                    \"sentences\": [cluster_sentences[i] for i, label in enumerate(hdbscan_labels) if label == hdbscan_cluster_id],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Append subdivided clusters to the remaining clusters\n",
    "    combined_clusters.extend(new_subclusters)\n",
    "\n",
    "    # Convert the combined clusters into a dataframe\n",
    "    new_cluster_df = pd.DataFrame(combined_clusters)\n",
    "    return new_cluster_df\n",
    "\n",
    "# Subdivide selected clusters and create a new dataframe\n",
    "new_cluster_df = subdivide_clusters_to_new_dataframe(\n",
    "    clustered_sentences, cluster_assignment, reduced_embeddings, clusters_to_subdivide\n",
    ")\n",
    "\n",
    "# Display the new dataframe\n",
    "print(new_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your desired output path\n",
    "new_cluster_df.to_csv(output_path_second_manual_check, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clean and name the final clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean by suppressing the clusters off topic, merge and finally name the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppress_and_merge_clusters(cluster_df, clusters_to_suppress, clusters_to_merge):\n",
    "    \"\"\"\n",
    "    Suppress and merge clusters in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - cluster_df: DataFrame containing cluster information.\n",
    "    - clusters_to_suppress: List of cluster IDs to suppress.\n",
    "    - clusters_to_merge: Dictionary where keys are clusters to keep, and values are lists of clusters to merge into them.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with suppressed and merged clusters.\n",
    "    \"\"\"\n",
    "    # Suppress clusters\n",
    "    suppressed_df = cluster_df[~cluster_df[\"cluster_id\"].isin(clusters_to_suppress)]\n",
    "\n",
    "    # Merge clusters\n",
    "    for target_cluster, clusters_to_merge_into in clusters_to_merge.items():\n",
    "        # Find the sentences for all clusters to merge\n",
    "        sentences_to_merge = []\n",
    "        for merge_cluster in clusters_to_merge_into:\n",
    "            merge_rows = suppressed_df[suppressed_df[\"cluster_id\"] == merge_cluster]\n",
    "            if not merge_rows.empty:\n",
    "                sentences_to_merge.extend(merge_rows.iloc[0][\"sentences\"])\n",
    "        \n",
    "        # Append sentences to the target cluster\n",
    "        target_row = suppressed_df[suppressed_df[\"cluster_id\"] == target_cluster]\n",
    "        if not target_row.empty:\n",
    "            target_row_index = target_row.index[0]\n",
    "            suppressed_df.at[target_row_index, \"sentences\"] = (\n",
    "                suppressed_df.at[target_row_index, \"sentences\"] + sentences_to_merge\n",
    "            )\n",
    "        \n",
    "        # Remove merged clusters\n",
    "        suppressed_df = suppressed_df[~suppressed_df[\"cluster_id\"].isin(clusters_to_merge_into)]\n",
    "    \n",
    "    # Reset the index for a clean DataFrame\n",
    "    suppressed_df.reset_index(drop=True, inplace=True)\n",
    "    return suppressed_df\n",
    "\n",
    "\n",
    "# Define clusters to suppress\n",
    "clusters_to_suppress = [\n",
    "    1, 8,\"17-1\", \"29-3\", \"86-2\"\n",
    "]\n",
    "\n",
    "# Define clusters to merge\n",
    "clusters_to_merge = {\n",
    "    18: [56],\n",
    "    82: [\"4-2\",\"4-4\",\"54-6\",\"61-2\",\"89-0\",\"44-1\"]\n",
    "}\n",
    "\n",
    "# Apply suppression and merging\n",
    "cleaned_cluster_df = suppress_and_merge_clusters(new_cluster_df, clusters_to_suppress, clusters_to_merge)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(cleaned_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_sentences(cluster_df):\n",
    "    \"\"\"\n",
    "    Removes duplicate sentences within each row of the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - cluster_df: DataFrame containing cluster information with a 'sentences' column.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with unique sentences in each cluster.\n",
    "    \"\"\"\n",
    "    # Ensure each row's \"sentences\" list contains only unique values\n",
    "    cluster_df[\"sentences\"] = cluster_df[\"sentences\"].apply(lambda x: list(set(x)))\n",
    "    return cluster_df\n",
    "\n",
    "\n",
    "# Apply the function to remove duplicates\n",
    "cleaned_cluster_df = remove_duplicate_sentences(cleaned_cluster_df)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(cleaned_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_cluster_names(cluster_df, cluster_name_mapping):\n",
    "    \"\"\"\n",
    "    Assign cluster names based on the provided mapping.\n",
    "\n",
    "    Parameters:\n",
    "    - cluster_df: DataFrame containing clusters.\n",
    "    - cluster_name_mapping: Dictionary mapping cluster_id to cluster names.\n",
    "\n",
    "    Returns:\n",
    "    - Updated DataFrame with a new column for cluster names.\n",
    "    \"\"\"\n",
    "    cluster_df = cluster_df.copy()  # Avoid modifying the original DataFrame\n",
    "\n",
    "    # Assign names using the mapping\n",
    "    cluster_df[\"Cluster Name\"] = cluster_df[\"cluster_id\"].map(cluster_name_mapping)\n",
    "\n",
    "    # Fill missing names with \"Unnamed Cluster\"\n",
    "    cluster_df[\"Cluster Name\"].fillna(\"Unnamed Cluster\", inplace=True)\n",
    "\n",
    "    return cluster_df\n",
    "\n",
    "\n",
    "# Define the cluster name mapping (shortened for brevity; use the full mapping provided)\n",
    "cluster_name_mapping = {\n",
    "    18: \"Access, Level and Quality of Service\",\n",
    "    78: \"Accessibility\"\n",
    "}\n",
    "\n",
    "# Apply the function to assign names\n",
    "named_cluster_df = assign_cluster_names(cleaned_cluster_df, cluster_name_mapping)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(named_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your desired output path\n",
    "named_cluster_df.to_csv(export_final_dataset_cluster, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Meta clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bigger clusters for outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = pd.read_csv(prepared_for_clustering)\n",
    "cluster_df = pd.read_csv(named_cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df=updated_df.dropna(subset='matched_cluster')\n",
    "updated_df = pd.merge(updated_df, cluster_df[['Cluster Name','Agg Cluster']], how= 'left', left_on= 'matched_cluster', right_on= 'Cluster Name')\n",
    "grouped = updated_df.groupby([\"Agg Cluster\",\"Cluster Name\"])[\"matched_cluster\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your desired output path\n",
    "grouped.to_csv(export_agg_cluster, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
