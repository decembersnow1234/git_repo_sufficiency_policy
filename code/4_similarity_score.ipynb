{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure of similarity of policy and abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code allows the calculation of similarity of policy and abstract.\n",
    "\n",
    "It uses the outputs of the other phases that will be used to compute the matrix of correlations pondered by a similarity score. The similarity score is computed using Proportional Sentence Match.\n",
    "\n",
    "In this Jupyter Notebook we will: \n",
    "1. Import the data retrieved from the policy and outcome clustering process ; \n",
    "2. Import the relevant packages ;\n",
    "3. Prepare data for computing ;\n",
    "4. Prevalence of policies in abstract Using Proportional Sentence Match ; \n",
    "5. Export data with prevalence measure.\n",
    "\n",
    "To complete those tasks you will need:\n",
    "- The dataset of papers with the policy extraction of the 1_policy_extraction code. \n",
    "- The dataset of papers with the clustered policy of the 2_policy_clustering code. \n",
    "- The dataset of papers with the clustered policy of the 3_outcomes_clustering code. \n",
    "\n",
    "At the end of this script you will extract: \n",
    "- The named_cluster_df dataset of policies with prevalence metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the data retrieved from the policy and outcome clustering process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the input and output access paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3 inputs\n",
    "## Output dataset of the 1_policy_extraction (df)\n",
    "input_path_article = \"\"\n",
    "## Output dataset of the 2_policy_clustering (named_cluster_df)\n",
    "input_path_policy = \"\"\n",
    "## Output dataset of the 3_outcomes_clustering (named_cluster_df)\n",
    "input_path_outcome = \"\"\n",
    "\n",
    "# 1 output\n",
    "## Final dataset with clusters\n",
    "policy_and_factors_clustered_similarity_normalized = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare data for computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_policy = pd.read_csv(\"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/5_final_db/3_policy_and_factors_clustered.csv\"  )\n",
    "df_init = pd.read_csv(\"C:/Users/easycash/Mon Drive/Thèse/1_Systematic mapping/6_structural_topic_model/3_exit/extract_policies_ML_concat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'index' to numeric, coercing errors to NaN\n",
    "df_policy['row_index'] = pd.to_numeric(df_policy['row_index'], errors='coerce')\n",
    "\n",
    "# Handle NaN values (e.g., drop rows with NaN)\n",
    "df_policy = df_policy.dropna(subset=['row_index'])\n",
    "df_policy = df_policy.dropna(subset=['matched_cluster_factor'])\n",
    "\n",
    "# Convert to integer\n",
    "df_policy['row_index'] = df_policy['row_index'].astype(int)\n",
    "\n",
    "data = df_policy.set_index('row_index').join(df_init.set_index('Index')['abstract'], how='left').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prevalence of policies in abstract Using Proportional Sentence Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Louis Notes:\n",
    "- Not clear to me what we are doing here: is it mapping for each cluster of policies the share of outcomes clusters that are similar (ex: 8/10 were positive, 1 neutral, 1 negative)?\n",
    "\"\"\"\n",
    "\n",
    "# Function to compute proportional sentence match\n",
    "def compute_proportional_sentence_match(df, threshold=0.8):\n",
    "    results = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        # Encode the abstract and POLICY sentences\n",
    "        abstract_embedding = model.encode(row['abstract'], convert_to_tensor=True)\n",
    "        policy_sentences = row['POLICY'].split(\".\")  # Split POLICY into individual sentences\n",
    "        policy_embeddings = model.encode(policy_sentences, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarities = util.cos_sim(policy_embeddings, abstract_embedding).numpy()\n",
    "        \n",
    "        # Metrics\n",
    "        #relevant_sentences = (similarities > threshold).sum()\n",
    "        #proportion_relevant = relevant_sentences / len(policy_sentences)\n",
    "        average_similarity = similarities.mean()\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'abstract': row['abstract'],\n",
    "            'POLICY': row['POLICY'],\n",
    "            #'proportion_relevant': proportion_relevant,\n",
    "            'average_similarity': average_similarity\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function\n",
    "results_df = compute_proportional_sentence_match(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_policy_similarity'] = results_df['average_similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['CORRELATION']=='increasing','CORRELATION_num'] = 1\n",
    "df.loc[df['CORRELATION']=='decreasing','CORRELATION_num'] = -1\n",
    "df.loc[df['CORRELATION']=='neutral','CORRELATION_num'] = 0\n",
    "\n",
    "# Normalize policy_similarity per matched_cluster\n",
    "df['policy_similarity_normalized_by_cluster'] = df.groupby('matched_cluster')['average_policy_similarity'] \\\n",
    "    .transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "# Normalize policy_similarity per matched_cluster_factor\n",
    "df['policy_similarity_normalized_by_factor'] = df.groupby('matched_cluster_factor')['average_policy_similarity'] \\\n",
    "    .transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "# If you want global normalization\n",
    "df['policy_similarity_normalized_global'] = (df['average_policy_similarity'] - df['average_policy_similarity'].min()) / \\\n",
    "                                            (df['average_policy_similarity'].max() - df['average_policy_similarity'].min())\n",
    "\n",
    "df['correlation_prod_normalized_by_cluster'] = df['policy_similarity_normalized_by_cluster']*df['CORRELATION_num']\n",
    "df['correlation_prod_normalized_by_factor'] = df['policy_similarity_normalized_by_factor']*df['CORRELATION_num']\n",
    "df['correlation_prod_normalized_global'] = df['policy_similarity_normalized_global']*df['CORRELATION_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export data with prevalence measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.read_csv(input_path_policy)\n",
    "cluster_df_factor = pd.read_csv(input_path_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df=df.dropna(subset='matched_cluster')\n",
    "updated_df = pd.merge(updated_df, cluster_df[['Cluster Name','Agg Cluster']], how= 'left', left_on= 'matched_cluster', right_on= 'Cluster Name')\n",
    "grouped = updated_df.groupby([\"Agg Cluster\",\"Cluster Name\"])[\"matched_cluster\"].count()\n",
    "\n",
    "updated_df = pd.merge(updated_df, cluster_df_factor[['Cluster Name','Agg Cluster','Corr Sign']], how= 'left', left_on= 'matched_cluster_factor', right_on= 'Cluster Name',suffixes=(False, '_factor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your desired output path\n",
    "updated_df.to_csv(policy_and_factors_clustered_similarity_normalized, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
